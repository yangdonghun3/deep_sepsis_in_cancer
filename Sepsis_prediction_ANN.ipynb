{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EHRs : \n",
      "1 fold\n",
      "epoch0\tloss:0.717\tacc:0.483 \tval_loss:0.714\tv_acc:0.519\n",
      "epoch100\tloss:0.59\tacc:0.722 \tval_loss:0.659\tv_acc:0.646\n",
      "epoch200\tloss:0.535\tacc:0.761 \tval_loss:0.652\tv_acc:0.646\n",
      "epoch300\tloss:0.509\tacc:0.787 \tval_loss:0.652\tv_acc:0.631\n",
      "epoch400\tloss:0.498\tacc:0.796 \tval_loss:0.652\tv_acc:0.631\n",
      "2 fold\n",
      "epoch0\tloss:0.728\tacc:0.478 \tval_loss:0.721\tv_acc:0.454\n",
      "epoch100\tloss:0.603\tacc:0.668 \tval_loss:0.676\tv_acc:0.573\n",
      "epoch200\tloss:0.548\tacc:0.756 \tval_loss:0.662\tv_acc:0.593\n",
      "epoch300\tloss:0.52\tacc:0.789 \tval_loss:0.656\tv_acc:0.613\n",
      "epoch400\tloss:0.51\tacc:0.797 \tval_loss:0.655\tv_acc:0.609\n",
      "3 fold\n",
      "epoch0\tloss:0.709\tacc:0.533 \tval_loss:0.698\tv_acc:0.526\n",
      "epoch100\tloss:0.586\tacc:0.696 \tval_loss:0.665\tv_acc:0.624\n",
      "epoch200\tloss:0.534\tacc:0.755 \tval_loss:0.669\tv_acc:0.634\n",
      "epoch300\tloss:0.508\tacc:0.776 \tval_loss:0.673\tv_acc:0.631\n",
      "epoch400\tloss:0.498\tacc:0.783 \tval_loss:0.675\tv_acc:0.634\n",
      "4 fold\n",
      "epoch0\tloss:0.772\tacc:0.434 \tval_loss:0.764\tv_acc:0.451\n",
      "epoch100\tloss:0.611\tacc:0.677 \tval_loss:0.715\tv_acc:0.549\n",
      "epoch200\tloss:0.552\tacc:0.741 \tval_loss:0.712\tv_acc:0.582\n",
      "epoch300\tloss:0.525\tacc:0.759 \tval_loss:0.712\tv_acc:0.588\n",
      "epoch400\tloss:0.514\tacc:0.769 \tval_loss:0.712\tv_acc:0.585\n",
      "5 fold\n",
      "epoch0\tloss:0.768\tacc:0.485 \tval_loss:0.746\tv_acc:0.518\n",
      "epoch100\tloss:0.611\tacc:0.66 \tval_loss:0.674\tv_acc:0.578\n",
      "epoch200\tloss:0.55\tacc:0.735 \tval_loss:0.667\tv_acc:0.604\n",
      "epoch300\tloss:0.522\tacc:0.76 \tval_loss:0.668\tv_acc:0.611\n",
      "epoch400\tloss:0.512\tacc:0.763 \tval_loss:0.669\tv_acc:0.614\n",
      "\n",
      "\n",
      "ANN : \n",
      "  Acc      :  0.615\n",
      "  AUROC    :  0.664\n",
      "  AUPRC    :  0.449\n",
      "  Precision:  0.44\n",
      "  Recall   :  0.7\n",
      "  F1-score :  0.54\n",
      "\n",
      "\n",
      "EHRs_DrugRel : \n",
      "1 fold\n",
      "epoch0\tloss:0.695\tacc:0.555 \tval_loss:0.703\tv_acc:0.593\n",
      "epoch100\tloss:0.485\tacc:0.81 \tval_loss:0.666\tv_acc:0.638\n",
      "epoch200\tloss:0.42\tacc:0.843 \tval_loss:0.667\tv_acc:0.638\n",
      "epoch300\tloss:0.388\tacc:0.857 \tval_loss:0.67\tv_acc:0.634\n",
      "epoch400\tloss:0.376\tacc:0.867 \tval_loss:0.671\tv_acc:0.634\n",
      "2 fold\n",
      "epoch0\tloss:0.752\tacc:0.425 \tval_loss:0.744\tv_acc:0.487\n",
      "epoch100\tloss:0.492\tacc:0.814 \tval_loss:0.648\tv_acc:0.629\n",
      "epoch200\tloss:0.426\tacc:0.841 \tval_loss:0.642\tv_acc:0.636\n",
      "epoch300\tloss:0.395\tacc:0.866 \tval_loss:0.642\tv_acc:0.626\n",
      "epoch400\tloss:0.382\tacc:0.874 \tval_loss:0.642\tv_acc:0.619\n",
      "3 fold\n",
      "epoch0\tloss:0.697\tacc:0.522 \tval_loss:0.714\tv_acc:0.474\n",
      "epoch100\tloss:0.486\tacc:0.796 \tval_loss:0.637\tv_acc:0.666\n",
      "epoch200\tloss:0.426\tacc:0.835 \tval_loss:0.641\tv_acc:0.679\n",
      "epoch300\tloss:0.396\tacc:0.853 \tval_loss:0.646\tv_acc:0.676\n",
      "epoch400\tloss:0.384\tacc:0.861 \tval_loss:0.648\tv_acc:0.672\n",
      "4 fold\n",
      "epoch0\tloss:0.743\tacc:0.473 \tval_loss:0.737\tv_acc:0.431\n",
      "epoch100\tloss:0.488\tacc:0.793 \tval_loss:0.699\tv_acc:0.552\n",
      "epoch200\tloss:0.417\tacc:0.853 \tval_loss:0.702\tv_acc:0.572\n",
      "epoch300\tloss:0.383\tacc:0.874 \tval_loss:0.708\tv_acc:0.585\n",
      "epoch400\tloss:0.37\tacc:0.879 \tval_loss:0.71\tv_acc:0.585\n",
      "5 fold\n",
      "epoch0\tloss:0.711\tacc:0.546 \tval_loss:0.754\tv_acc:0.512\n",
      "epoch100\tloss:0.491\tacc:0.804 \tval_loss:0.663\tv_acc:0.601\n",
      "epoch200\tloss:0.422\tacc:0.859 \tval_loss:0.658\tv_acc:0.591\n",
      "epoch300\tloss:0.39\tacc:0.877 \tval_loss:0.66\tv_acc:0.594\n",
      "epoch400\tloss:0.377\tacc:0.886 \tval_loss:0.662\tv_acc:0.594\n",
      "\n",
      "\n",
      "ANN : \n",
      "  Acc      :  0.622\n",
      "  AUROC    :  0.698\n",
      "  AUPRC    :  0.506\n",
      "  Precision:  0.447\n",
      "  Recall   :  0.698\n",
      "  F1-score :  0.545\n",
      "\n",
      "\n",
      "EHRs_DrugRel_Lab : \n",
      "1 fold\n",
      "epoch0\tloss:0.766\tacc:0.45 \tval_loss:0.722\tv_acc:0.511\n",
      "epoch100\tloss:0.483\tacc:0.811 \tval_loss:0.654\tv_acc:0.668\n",
      "epoch200\tloss:0.418\tacc:0.852 \tval_loss:0.663\tv_acc:0.638\n",
      "epoch300\tloss:0.386\tacc:0.875 \tval_loss:0.668\tv_acc:0.646\n",
      "epoch400\tloss:0.374\tacc:0.88 \tval_loss:0.67\tv_acc:0.653\n",
      "2 fold\n",
      "epoch0\tloss:0.764\tacc:0.475 \tval_loss:0.774\tv_acc:0.47\n",
      "epoch100\tloss:0.498\tacc:0.799 \tval_loss:0.655\tv_acc:0.606\n",
      "epoch200\tloss:0.432\tacc:0.848 \tval_loss:0.646\tv_acc:0.636\n",
      "epoch300\tloss:0.399\tacc:0.873 \tval_loss:0.645\tv_acc:0.642\n",
      "epoch400\tloss:0.387\tacc:0.881 \tval_loss:0.645\tv_acc:0.639\n",
      "3 fold\n",
      "epoch0\tloss:0.724\tacc:0.517 \tval_loss:0.743\tv_acc:0.47\n",
      "epoch100\tloss:0.488\tacc:0.801 \tval_loss:0.649\tv_acc:0.62\n",
      "epoch200\tloss:0.417\tacc:0.856 \tval_loss:0.649\tv_acc:0.652\n",
      "epoch300\tloss:0.382\tacc:0.874 \tval_loss:0.653\tv_acc:0.655\n",
      "epoch400\tloss:0.369\tacc:0.878 \tval_loss:0.656\tv_acc:0.659\n",
      "4 fold\n",
      "epoch0\tloss:0.709\tacc:0.548 \tval_loss:0.711\tv_acc:0.507\n",
      "epoch100\tloss:0.463\tacc:0.816 \tval_loss:0.662\tv_acc:0.621\n",
      "epoch200\tloss:0.395\tacc:0.864 \tval_loss:0.656\tv_acc:0.644\n",
      "epoch300\tloss:0.362\tacc:0.886 \tval_loss:0.658\tv_acc:0.66\n",
      "epoch400\tloss:0.349\tacc:0.888 \tval_loss:0.658\tv_acc:0.65\n",
      "5 fold\n",
      "epoch0\tloss:0.742\tacc:0.496 \tval_loss:0.766\tv_acc:0.475\n",
      "epoch100\tloss:0.483\tacc:0.815 \tval_loss:0.656\tv_acc:0.614\n",
      "epoch200\tloss:0.414\tacc:0.866 \tval_loss:0.65\tv_acc:0.657\n",
      "epoch300\tloss:0.381\tacc:0.887 \tval_loss:0.651\tv_acc:0.657\n",
      "epoch400\tloss:0.369\tacc:0.898 \tval_loss:0.652\tv_acc:0.667\n",
      "\n",
      "\n",
      "ANN : \n",
      "  Acc      :  0.654\n",
      "  AUROC    :  0.712\n",
      "  AUPRC    :  0.514\n",
      "  Precision:  0.477\n",
      "  Recall   :  0.723\n",
      "  F1-score :  0.575\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from utils.Dataset import Dataset\n",
    "from net.networks import ann\n",
    "import random\n",
    "import torch.random\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score,average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "EHRs_DrugRel_Lab = pd.read_csv(\"preprocessed_data(dummy)/EHRs_DrugRel_Lab.csv\")\n",
    "Lab_col=EHRs_DrugRel_Lab.columns[101:136]\n",
    "DrugRel_col=EHRs_DrugRel_Lab.columns[346:1741]\n",
    "EHRs_DrugRel=EHRs_DrugRel_Lab.drop(Lab_col.values,axis=1)\n",
    "EHRs=EHRs_DrugRel.drop(DrugRel_col,axis=1)\n",
    "\n",
    "tc=[[1,2],[3,4],[5,6],[7,8],[9,0]]\n",
    "data={\"EHRs\":EHRs,\"EHRs_DrugRel\":EHRs_DrugRel,\"EHRs_DrugRel_Lab\":EHRs_DrugRel_Lab}\n",
    "\n",
    "for d in data.keys():\n",
    "    data_df=data[d]\n",
    "    print(d,\": \")\n",
    "    data_df=data_df.drop([\"Sepsis_Date\"],axis=1)\n",
    "    \n",
    "    ann_acc = []\n",
    "    ann_roc = []\n",
    "    ann_prc = []\n",
    "    ann_pre = []\n",
    "    ann_rec = []\n",
    "    ann_f1  = []\n",
    "    \n",
    "    for tc_1, tc_2 in tc:\n",
    "        print(int(tc_1/2)+1,\"fold\")\n",
    "        padding = pd.DataFrame(0*np.ones((len(data_df), 1742-len(data_df.columns)-1)))\n",
    "        data_df = pd.concat([data_df,padding],axis=1)    \n",
    "        train_data=data_df.loc[(data_df[\"PT_ID\"]%10!=tc_1) & (data_df[\"PT_ID\"]%10!=tc_2)]\n",
    "        test_data=data_df.loc[(data_df[\"PT_ID\"]%10==tc_1) |(data_df[\"PT_ID\"]%10==tc_2)]\n",
    "        train_feature = train_data.drop([\"Label\"], axis=1)\n",
    "        train_label = train_data[[\"Label\"]]\n",
    "        test_feature = test_data.drop([\"Label\"], axis=1)\n",
    "        test_label = test_data[[\"Label\"]]    \n",
    "        scaler = MinMaxScaler()\n",
    "        train_feature = scaler.fit_transform(train_feature)\n",
    "        test_feature = scaler.transform(test_feature)\n",
    "        rd = RandomUnderSampler()\n",
    "        train_feature, train_label = rd.fit_resample(train_feature,train_label)\n",
    "    \n",
    "        BATCH_SIZE=int(len(train_feature))\n",
    "        n_epochs=500    \n",
    "        LEARNING_RATE= 0.000005\n",
    "        \n",
    "        model = ann(in_size=len(train_feature[0]),h_size=1024)\n",
    "        model=model.cuda()\n",
    "        \n",
    "        train_data = Dataset(torch.FloatTensor(train_feature), torch.FloatTensor(train_label.values))\n",
    "        test_data = Dataset(torch.FloatTensor(test_feature), torch.FloatTensor(test_label.values))\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=len(test_feature), shuffle=False)\n",
    "        \n",
    "        loss_f = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs*len(train_loader), eta_min=0)\n",
    "        \n",
    "        losses = []\n",
    "        accur = []\n",
    "        val_losses = []\n",
    "        val_accur = []    \n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            #if i==500:\n",
    "            #    break;\n",
    "            total_loss = 0\n",
    "            total_acc = 0 \n",
    "            val_total_loss = 0\n",
    "            val_total_acc = 0 \n",
    "            \n",
    "            for j,(x_train,y_train) in enumerate(train_loader):            \n",
    "                x_train,y_train=x_train.cuda(),y_train.cuda()            \n",
    "                output = model(x_train)\n",
    "                loss = loss_f(output,y_train.reshape(-1,1))\n",
    "                acc = (torch.round(output.reshape(-1)) == y_train.reshape(-1)).sum()/len(y_train)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss+=loss.item()\n",
    "                total_acc+=acc.item()\n",
    "                \n",
    "            with torch.no_grad():    \n",
    "                for j,(x_test,y_test) in enumerate(test_loader):\n",
    "                    x_test,y_test=x_test.cuda(),y_test.cuda()            \n",
    "                    y_pre = model(x_test)\n",
    "                    val_loss = loss_f(y_pre,y_test.reshape(-1,1))\n",
    "                    val_acc=(torch.round(y_pre.reshape(-1)) == y_test.reshape(-1)).sum()/len(y_test)    \n",
    "                    val_total_loss+=val_loss.item()\n",
    "                    val_total_acc+=val_acc.item()\n",
    "            total_loss = total_loss/len(train_loader)\n",
    "            total_acc = total_acc/len(train_loader)\n",
    "            val_total_loss = val_total_loss/len(test_loader)\n",
    "            val_total_acc = val_total_acc/len(test_loader)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                losses.append(loss)\n",
    "                accur.append(acc)\n",
    "                val_losses.append(val_loss)\n",
    "                #val_accur.append(val_acc)        \n",
    "                #print(\"epoch {}\\tloss : {}\\t acc : {}\".format(i,loss,acc),\"\\t val_loss : {}\\t v_acc : {}\".format(val_loss,val_acc))\n",
    "                print(\"epoch{}\\tloss:{}\\tacc:{}\"\n",
    "                      .format(i,np.round(total_loss,3),np.round(total_acc,3)),\n",
    "                      \"\\tval_loss:{}\\tv_acc:{}\"\n",
    "                      .format(np.round(val_total_loss,3),np.round(val_total_acc,3)))\n",
    "                \n",
    "        y_pre=torch.round(model(torch.cuda.FloatTensor(test_feature))).cpu().detach().numpy()\n",
    "        y_proba=model(torch.cuda.FloatTensor(test_feature)).cpu().detach().numpy()\n",
    "        \n",
    "        y_label = test_label.values\n",
    "        \n",
    "        test_acc  = (y_pre==y_label).sum()/len(y_label)\n",
    "        AUROC     = roc_auc_score(y_label, y_proba)\n",
    "        AUPRC     = average_precision_score(y_label, y_proba)\n",
    "        precision = precision_score(y_label, y_pre, pos_label=1)\n",
    "        recall    = recall_score(y_label, y_pre)\n",
    "        f1_score_ = f1_score(y_label, y_pre)\n",
    "        \n",
    "        ann_acc = ann_acc + [test_acc]\n",
    "        ann_roc = ann_roc + [AUROC]\n",
    "        ann_prc = ann_prc + [AUPRC]\n",
    "        ann_pre = ann_pre + [precision]\n",
    "        ann_rec = ann_rec + [recall]\n",
    "        ann_f1  = ann_f1  + [f1_score_]\n",
    "        \n",
    "        #torch.save(model.state_dict(), \"trained_model/\"+d+\"_\"+str(tc_1)+\"_ann.pt\")\n",
    "        #joblib.dump(scaler, \"trained_model/\"+d+\"_\"+str(tc_1)+\"_scaler.pkl\")        \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"ANN : \")\n",
    "    print(\"  Acc      : \",np.round(np.array(ann_acc).mean(),3))\n",
    "    print(\"  AUROC    : \",np.round(np.array(ann_roc).mean(),3))\n",
    "    print(\"  AUPRC    : \",np.round(np.array(ann_prc).mean(),3))\n",
    "    print(\"  Precision: \",np.round(np.array(ann_pre).mean(),3))\n",
    "    print(\"  Recall   : \",np.round(np.array(ann_rec).mean(),3))\n",
    "    print(\"  F1-score : \",np.round(np.array(ann_f1 ).mean(),3))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaist_hc",
   "language": "python",
   "name": "kaist_hc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
