{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EHRs : \n",
      "1 fold\n",
      "epoch0\tloss:0.695\tacc:0.5 \tval_loss:0.709\tv_acc:0.307\n",
      "epoch100\tloss:0.691\tacc:0.5 \tval_loss:0.701\tv_acc:0.307\n",
      "epoch200\tloss:0.687\tacc:0.607 \tval_loss:0.695\tv_acc:0.492\n",
      "epoch300\tloss:0.682\tacc:0.673 \tval_loss:0.689\tv_acc:0.591\n",
      "epoch400\tloss:0.677\tacc:0.677 \tval_loss:0.684\tv_acc:0.661\n",
      "epoch500\tloss:0.67\tacc:0.668 \tval_loss:0.677\tv_acc:0.657\n",
      "epoch600\tloss:0.661\tacc:0.664 \tval_loss:0.671\tv_acc:0.665\n",
      "epoch700\tloss:0.652\tacc:0.673 \tval_loss:0.668\tv_acc:0.665\n",
      "epoch800\tloss:0.644\tacc:0.677 \tval_loss:0.666\tv_acc:0.673\n",
      "2 fold\n",
      "epoch0\tloss:0.692\tacc:0.5 \tval_loss:0.701\tv_acc:0.324\n",
      "epoch100\tloss:0.689\tacc:0.618 \tval_loss:0.692\tv_acc:0.537\n",
      "epoch200\tloss:0.685\tacc:0.662 \tval_loss:0.687\tv_acc:0.626\n",
      "epoch300\tloss:0.681\tacc:0.649 \tval_loss:0.681\tv_acc:0.633\n",
      "epoch400\tloss:0.675\tacc:0.651 \tval_loss:0.675\tv_acc:0.644\n",
      "epoch500\tloss:0.668\tacc:0.662 \tval_loss:0.667\tv_acc:0.651\n",
      "epoch600\tloss:0.659\tacc:0.661 \tval_loss:0.659\tv_acc:0.662\n",
      "epoch700\tloss:0.651\tacc:0.656 \tval_loss:0.654\tv_acc:0.655\n",
      "epoch800\tloss:0.643\tacc:0.659 \tval_loss:0.65\tv_acc:0.651\n",
      "3 fold\n",
      "epoch0\tloss:0.693\tacc:0.494 \tval_loss:0.698\tv_acc:0.338\n",
      "epoch100\tloss:0.69\tacc:0.639 \tval_loss:0.693\tv_acc:0.513\n",
      "epoch200\tloss:0.686\tacc:0.67 \tval_loss:0.689\tv_acc:0.574\n",
      "epoch300\tloss:0.682\tacc:0.676 \tval_loss:0.686\tv_acc:0.589\n",
      "epoch400\tloss:0.675\tacc:0.676 \tval_loss:0.681\tv_acc:0.589\n",
      "epoch500\tloss:0.666\tacc:0.673 \tval_loss:0.677\tv_acc:0.57\n",
      "epoch600\tloss:0.656\tacc:0.668 \tval_loss:0.675\tv_acc:0.551\n",
      "epoch700\tloss:0.647\tacc:0.672 \tval_loss:0.676\tv_acc:0.555\n",
      "epoch800\tloss:0.641\tacc:0.676 \tval_loss:0.677\tv_acc:0.551\n",
      "4 fold\n",
      "epoch0\tloss:0.693\tacc:0.507 \tval_loss:0.689\tv_acc:0.651\n",
      "epoch100\tloss:0.689\tacc:0.607 \tval_loss:0.69\tv_acc:0.595\n",
      "epoch200\tloss:0.686\tacc:0.663 \tval_loss:0.689\tv_acc:0.592\n",
      "epoch300\tloss:0.681\tacc:0.673 \tval_loss:0.688\tv_acc:0.55\n",
      "epoch400\tloss:0.675\tacc:0.661 \tval_loss:0.687\tv_acc:0.533\n",
      "epoch500\tloss:0.668\tacc:0.661 \tval_loss:0.685\tv_acc:0.533\n",
      "epoch600\tloss:0.661\tacc:0.654 \tval_loss:0.684\tv_acc:0.526\n",
      "epoch700\tloss:0.653\tacc:0.653 \tval_loss:0.684\tv_acc:0.519\n",
      "epoch800\tloss:0.647\tacc:0.648 \tval_loss:0.684\tv_acc:0.519\n",
      "5 fold\n",
      "epoch0\tloss:0.694\tacc:0.5 \tval_loss:0.708\tv_acc:0.335\n",
      "epoch100\tloss:0.69\tacc:0.527 \tval_loss:0.696\tv_acc:0.378\n",
      "epoch200\tloss:0.686\tacc:0.636 \tval_loss:0.689\tv_acc:0.54\n",
      "epoch300\tloss:0.681\tacc:0.656 \tval_loss:0.684\tv_acc:0.586\n",
      "epoch400\tloss:0.675\tacc:0.652 \tval_loss:0.678\tv_acc:0.597\n",
      "epoch500\tloss:0.667\tacc:0.646 \tval_loss:0.675\tv_acc:0.586\n",
      "epoch600\tloss:0.658\tacc:0.654 \tval_loss:0.674\tv_acc:0.586\n",
      "epoch700\tloss:0.651\tacc:0.661 \tval_loss:0.674\tv_acc:0.583\n",
      "epoch800\tloss:0.645\tacc:0.663 \tval_loss:0.673\tv_acc:0.59\n",
      "\n",
      "\n",
      "lstm : \n",
      "  Acc      :  0.598\n",
      "  AUROC    :  0.658\n",
      "  AUPRC    :  0.454\n",
      "  Precision:  0.423\n",
      "  Recall   :  0.659\n",
      "  F1-score :  0.513\n",
      "\n",
      "\n",
      "EHRs_DrugRel : \n",
      "1 fold\n",
      "epoch0\tloss:0.694\tacc:0.493 \tval_loss:0.698\tv_acc:0.457\n",
      "epoch100\tloss:0.654\tacc:0.675 \tval_loss:0.668\tv_acc:0.646\n",
      "epoch200\tloss:0.608\tacc:0.692 \tval_loss:0.64\tv_acc:0.657\n",
      "epoch300\tloss:0.57\tacc:0.719 \tval_loss:0.63\tv_acc:0.665\n",
      "epoch400\tloss:0.539\tacc:0.738 \tval_loss:0.622\tv_acc:0.665\n",
      "epoch500\tloss:0.515\tacc:0.746 \tval_loss:0.615\tv_acc:0.697\n",
      "epoch600\tloss:0.497\tacc:0.755 \tval_loss:0.613\tv_acc:0.693\n",
      "epoch700\tloss:0.484\tacc:0.76 \tval_loss:0.615\tv_acc:0.693\n",
      "epoch800\tloss:0.474\tacc:0.766 \tval_loss:0.618\tv_acc:0.701\n",
      "2 fold\n",
      "epoch0\tloss:0.697\tacc:0.493 \tval_loss:0.711\tv_acc:0.331\n",
      "epoch100\tloss:0.658\tacc:0.676 \tval_loss:0.66\tv_acc:0.641\n",
      "epoch200\tloss:0.613\tacc:0.697 \tval_loss:0.621\tv_acc:0.655\n",
      "epoch300\tloss:0.574\tacc:0.73 \tval_loss:0.603\tv_acc:0.68\n",
      "epoch400\tloss:0.541\tacc:0.748 \tval_loss:0.6\tv_acc:0.708\n",
      "epoch500\tloss:0.516\tacc:0.762 \tval_loss:0.604\tv_acc:0.708\n",
      "epoch600\tloss:0.498\tacc:0.77 \tval_loss:0.608\tv_acc:0.708\n",
      "epoch700\tloss:0.485\tacc:0.777 \tval_loss:0.618\tv_acc:0.701\n",
      "epoch800\tloss:0.475\tacc:0.782 \tval_loss:0.626\tv_acc:0.701\n",
      "3 fold\n",
      "epoch0\tloss:0.695\tacc:0.501 \tval_loss:0.692\tv_acc:0.574\n",
      "epoch100\tloss:0.654\tacc:0.668 \tval_loss:0.677\tv_acc:0.616\n",
      "epoch200\tloss:0.604\tacc:0.683 \tval_loss:0.664\tv_acc:0.62\n",
      "epoch300\tloss:0.561\tacc:0.72 \tval_loss:0.663\tv_acc:0.631\n",
      "epoch400\tloss:0.53\tacc:0.742 \tval_loss:0.67\tv_acc:0.624\n",
      "epoch500\tloss:0.507\tacc:0.762 \tval_loss:0.677\tv_acc:0.643\n",
      "epoch600\tloss:0.491\tacc:0.76 \tval_loss:0.685\tv_acc:0.639\n",
      "epoch700\tloss:0.478\tacc:0.774 \tval_loss:0.693\tv_acc:0.639\n",
      "epoch800\tloss:0.469\tacc:0.784 \tval_loss:0.701\tv_acc:0.635\n",
      "4 fold\n",
      "epoch0\tloss:0.7\tacc:0.5 \tval_loss:0.671\tv_acc:0.657\n",
      "epoch100\tloss:0.66\tacc:0.682 \tval_loss:0.677\tv_acc:0.585\n",
      "epoch200\tloss:0.618\tacc:0.715 \tval_loss:0.654\tv_acc:0.623\n",
      "epoch300\tloss:0.576\tacc:0.731 \tval_loss:0.637\tv_acc:0.64\n",
      "epoch400\tloss:0.54\tacc:0.755 \tval_loss:0.629\tv_acc:0.64\n",
      "epoch500\tloss:0.513\tacc:0.765 \tval_loss:0.627\tv_acc:0.644\n",
      "epoch600\tloss:0.493\tacc:0.774 \tval_loss:0.629\tv_acc:0.651\n",
      "epoch700\tloss:0.479\tacc:0.78 \tval_loss:0.631\tv_acc:0.651\n",
      "epoch800\tloss:0.468\tacc:0.787 \tval_loss:0.634\tv_acc:0.647\n",
      "5 fold\n",
      "epoch0\tloss:0.695\tacc:0.49 \tval_loss:0.704\tv_acc:0.349\n",
      "epoch100\tloss:0.651\tacc:0.643 \tval_loss:0.673\tv_acc:0.543\n",
      "epoch200\tloss:0.604\tacc:0.681 \tval_loss:0.651\tv_acc:0.583\n",
      "epoch300\tloss:0.567\tacc:0.71 \tval_loss:0.637\tv_acc:0.604\n",
      "epoch400\tloss:0.538\tacc:0.734 \tval_loss:0.63\tv_acc:0.608\n",
      "epoch500\tloss:0.517\tacc:0.745 \tval_loss:0.629\tv_acc:0.604\n",
      "epoch600\tloss:0.499\tacc:0.758 \tval_loss:0.629\tv_acc:0.615\n",
      "epoch700\tloss:0.486\tacc:0.762 \tval_loss:0.632\tv_acc:0.608\n",
      "epoch800\tloss:0.476\tacc:0.771 \tval_loss:0.635\tv_acc:0.615\n",
      "\n",
      "\n",
      "lstm : \n",
      "  Acc      :  0.659\n",
      "  AUROC    :  0.712\n",
      "  AUPRC    :  0.532\n",
      "  Precision:  0.486\n",
      "  Recall   :  0.642\n",
      "  F1-score :  0.552\n",
      "\n",
      "\n",
      "EHRs_DrugRel_Lab : \n",
      "1 fold\n",
      "epoch0\tloss:0.693\tacc:0.51 \tval_loss:0.704\tv_acc:0.335\n",
      "epoch100\tloss:0.648\tacc:0.689 \tval_loss:0.672\tv_acc:0.618\n",
      "epoch200\tloss:0.595\tacc:0.711 \tval_loss:0.649\tv_acc:0.642\n",
      "epoch300\tloss:0.554\tacc:0.727 \tval_loss:0.641\tv_acc:0.642\n",
      "epoch400\tloss:0.524\tacc:0.753 \tval_loss:0.637\tv_acc:0.661\n",
      "epoch500\tloss:0.501\tacc:0.755 \tval_loss:0.635\tv_acc:0.665\n",
      "epoch600\tloss:0.483\tacc:0.765 \tval_loss:0.636\tv_acc:0.693\n",
      "epoch700\tloss:0.47\tacc:0.77 \tval_loss:0.639\tv_acc:0.697\n",
      "epoch800\tloss:0.461\tacc:0.784 \tval_loss:0.645\tv_acc:0.697\n",
      "2 fold\n",
      "epoch0\tloss:0.693\tacc:0.514 \tval_loss:0.696\tv_acc:0.413\n",
      "epoch100\tloss:0.648\tacc:0.665 \tval_loss:0.658\tv_acc:0.665\n",
      "epoch200\tloss:0.6\tacc:0.696 \tval_loss:0.627\tv_acc:0.673\n",
      "epoch300\tloss:0.561\tacc:0.728 \tval_loss:0.615\tv_acc:0.705\n",
      "epoch400\tloss:0.529\tacc:0.751 \tval_loss:0.612\tv_acc:0.712\n",
      "epoch500\tloss:0.503\tacc:0.775 \tval_loss:0.612\tv_acc:0.712\n",
      "epoch600\tloss:0.483\tacc:0.792 \tval_loss:0.614\tv_acc:0.726\n",
      "epoch700\tloss:0.468\tacc:0.797 \tval_loss:0.618\tv_acc:0.719\n",
      "epoch800\tloss:0.458\tacc:0.806 \tval_loss:0.622\tv_acc:0.712\n",
      "3 fold\n",
      "epoch0\tloss:0.697\tacc:0.5 \tval_loss:0.661\tv_acc:0.677\n",
      "epoch100\tloss:0.654\tacc:0.672 \tval_loss:0.664\tv_acc:0.616\n",
      "epoch200\tloss:0.607\tacc:0.709 \tval_loss:0.645\tv_acc:0.635\n",
      "epoch300\tloss:0.566\tacc:0.73 \tval_loss:0.642\tv_acc:0.654\n",
      "epoch400\tloss:0.534\tacc:0.749 \tval_loss:0.647\tv_acc:0.658\n",
      "epoch500\tloss:0.51\tacc:0.759 \tval_loss:0.651\tv_acc:0.654\n",
      "epoch600\tloss:0.492\tacc:0.77 \tval_loss:0.655\tv_acc:0.662\n",
      "epoch700\tloss:0.477\tacc:0.781 \tval_loss:0.66\tv_acc:0.665\n",
      "epoch800\tloss:0.466\tacc:0.787 \tval_loss:0.666\tv_acc:0.665\n",
      "4 fold\n",
      "epoch0\tloss:0.695\tacc:0.464 \tval_loss:0.696\tv_acc:0.384\n",
      "epoch100\tloss:0.651\tacc:0.669 \tval_loss:0.682\tv_acc:0.557\n",
      "epoch200\tloss:0.602\tacc:0.699 \tval_loss:0.662\tv_acc:0.619\n",
      "epoch300\tloss:0.558\tacc:0.713 \tval_loss:0.646\tv_acc:0.612\n",
      "epoch400\tloss:0.523\tacc:0.749 \tval_loss:0.639\tv_acc:0.623\n",
      "epoch500\tloss:0.495\tacc:0.767 \tval_loss:0.637\tv_acc:0.637\n",
      "epoch600\tloss:0.474\tacc:0.784 \tval_loss:0.639\tv_acc:0.637\n",
      "epoch700\tloss:0.459\tacc:0.798 \tval_loss:0.644\tv_acc:0.637\n",
      "epoch800\tloss:0.448\tacc:0.804 \tval_loss:0.65\tv_acc:0.633\n",
      "5 fold\n",
      "epoch0\tloss:0.696\tacc:0.441 \tval_loss:0.693\tv_acc:0.561\n",
      "epoch100\tloss:0.656\tacc:0.674 \tval_loss:0.668\tv_acc:0.59\n",
      "epoch200\tloss:0.609\tacc:0.697 \tval_loss:0.637\tv_acc:0.604\n",
      "epoch300\tloss:0.567\tacc:0.729 \tval_loss:0.615\tv_acc:0.64\n",
      "epoch400\tloss:0.533\tacc:0.745 \tval_loss:0.604\tv_acc:0.644\n",
      "epoch500\tloss:0.507\tacc:0.752 \tval_loss:0.599\tv_acc:0.655\n",
      "epoch600\tloss:0.487\tacc:0.759 \tval_loss:0.598\tv_acc:0.651\n",
      "epoch700\tloss:0.473\tacc:0.766 \tval_loss:0.598\tv_acc:0.647\n",
      "epoch800\tloss:0.462\tacc:0.773 \tval_loss:0.601\tv_acc:0.655\n",
      "\n",
      "\n",
      "lstm : \n",
      "  Acc      :  0.675\n",
      "  AUROC    :  0.727\n",
      "  AUPRC    :  0.547\n",
      "  Precision:  0.504\n",
      "  Recall   :  0.661\n",
      "  F1-score :  0.571\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from utils.Dataset import Dataset\n",
    "from net.networks import lstm\n",
    "import random\n",
    "import torch.random\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score,average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "EHRs_DrugRel_Lab = pd.read_csv(\"preprocessed_data(dummy)/rnn_EHRs_DrugRel_Lab.csv\")\n",
    "Lab_col=EHRs_DrugRel_Lab.columns[101:136]\n",
    "DrugRel_col=EHRs_DrugRel_Lab.columns[346:1741]\n",
    "EHRs_DrugRel=EHRs_DrugRel_Lab.drop(Lab_col.values,axis=1)\n",
    "EHRs=EHRs_DrugRel.drop(DrugRel_col,axis=1)\n",
    "\n",
    "tc=[[1,2],[3,4],[5,6],[7,8],[9,0]]\n",
    "data={\"EHRs\":EHRs,\"EHRs_DrugRel\":EHRs_DrugRel,\"EHRs_DrugRel_Lab\":EHRs_DrugRel_Lab}\n",
    "\n",
    "for d in data.keys():\n",
    "    data_df=data[d]\n",
    "    print(d,\": \")\n",
    "    \n",
    "    lstm_acc = []\n",
    "    lstm_roc = []\n",
    "    lstm_prc = []\n",
    "    lstm_pre = []\n",
    "    lstm_rec = []\n",
    "    lstm_f1  = []\n",
    "    \n",
    "    label_df = data_df.drop_duplicates([\"PT_ID\",\"Sepsis_Date\"],keep=\"last\")\n",
    "    padding = pd.DataFrame(0*np.ones((len(data_df), 1742-len(data_df.columns)+2)))\n",
    "    data_df = pd.concat([data_df,padding],axis=1)\n",
    "    \n",
    "    def padder (group):\n",
    "        padding=pd.DataFrame(np.append(np.ones((6-len(group), 1)) * group[\"PT_ID\"].values[0],\n",
    "                                       0*np.ones((6-len(group), len(data_df.columns) -1)),\n",
    "                                   axis=1),\n",
    "                             columns=data_df.columns)\n",
    "        return pd.concat([padding,group],axis=0)\n",
    "    \n",
    "    data_df = data_df.groupby(['PT_ID','Sepsis_Date']).apply(padder).reset_index(drop=True)    \n",
    "    \n",
    "    for tc_1, tc_2 in tc:\n",
    "        print(int(tc_1/2)+1,\"fold\")        \n",
    "        \n",
    "        train_data=data_df.loc[(data_df[\"PT_ID\"]%10!=tc_1) & (data_df[\"PT_ID\"]%10!=tc_2)]\n",
    "        test_data=data_df.loc[(data_df[\"PT_ID\"]%10==tc_1) |(data_df[\"PT_ID\"]%10==tc_2)]\n",
    "        train_feature = train_data.drop([\"Label\",\"Sepsis_Date\"], axis=1)\n",
    "        test_feature = test_data.drop([\"Label\",\"Sepsis_Date\"], axis=1)        \n",
    "        train_label_df=label_df.loc[(label_df[\"PT_ID\"]%10!=tc_1) & (label_df[\"PT_ID\"]%10!=tc_2)]\n",
    "        test_label_df=label_df.loc[(label_df[\"PT_ID\"]%10==tc_1) |(label_df[\"PT_ID\"]%10==tc_2)]\n",
    "        train_label = train_label_df[[\"Label\"]]\n",
    "        test_label = test_label_df[[\"Label\"]]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        train_feature = scaler.fit_transform(train_feature)\n",
    "        test_feature = scaler.transform(test_feature)        \n",
    "        train_feature=train_feature.reshape(-1, 6, 1742)\n",
    "        test_feature=test_feature.reshape(-1, 6, 1742)        \n",
    "        train_feature=np.reshape(train_feature, (-1, 1742*6))\n",
    "        rd = RandomUnderSampler()\n",
    "        train_feature, train_label = rd.fit_resample(train_feature,train_label)\n",
    "        train_feature=np.reshape(train_feature, (-1,6,1742))\n",
    "        \n",
    "        BATCH_SIZE=int(len(train_feature)/2)\n",
    "        in_size= test_feature.shape[2]\n",
    "        h_size=256\n",
    "        n_epochs=1500\n",
    "        LEARNING_RATE=0.00001\n",
    "        \n",
    "        train_data = Dataset(torch.FloatTensor(train_feature), torch.FloatTensor(train_label.values))\n",
    "        test_data = Dataset(torch.FloatTensor(test_feature), torch.FloatTensor(test_label.values))\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=len(test_feature), shuffle=False)\n",
    "        \n",
    "        model = lstm(in_size=in_size,h_size=h_size)\n",
    "        model=model.cuda()\n",
    "        loss_f = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs*len(train_loader), eta_min=0)\n",
    "\n",
    "        #forward loop\n",
    "        losses = []\n",
    "        accur = []\n",
    "        val_losses = []\n",
    "        val_accur = []       \n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            if i == 850:\n",
    "                break;\n",
    "            total_loss = 0\n",
    "            total_acc = 0 \n",
    "            val_total_loss = 0\n",
    "            val_total_acc = 0 \n",
    "            for j,(x_train,y_train) in enumerate(train_loader):\n",
    "                x_train,y_train=x_train.cuda(),y_train.cuda()\n",
    "                h = torch.zeros(1, len(x_train), h_size, requires_grad=True)\n",
    "                h = h.cuda()\n",
    "                c = torch.zeros(1, len(x_train), h_size, requires_grad=True)\n",
    "                c = h.cuda()\n",
    "                output = model(x_train,(h,c))\n",
    "                loss = loss_f(output,y_train.reshape(-1,1))        \n",
    "                acc = (torch.round(output.reshape(-1)) == y_train.reshape(-1)).sum()/len(y_train)\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss+=loss.item()\n",
    "                total_acc+=acc.item()\n",
    "                \n",
    "            with torch.no_grad():    \n",
    "                for j,(x_test,y_test) in enumerate(test_loader):\n",
    "                    x_test,y_test=x_test.cuda(),y_test.cuda()            \n",
    "                    h = torch.zeros(1, len(x_test), h_size, requires_grad=False)\n",
    "                    h = h.cuda()            \n",
    "                    c = torch.zeros(1, len(x_test), h_size, requires_grad=False)\n",
    "                    c = h.cuda()            \n",
    "                    y_pre = model(x_test,(h,c))\n",
    "                    val_loss = loss_f(y_pre,y_test.reshape(-1,1))                    \n",
    "                    val_acc=(torch.round(y_pre.reshape(-1)) == y_test.reshape(-1)).sum()/len(y_test)\n",
    "                    val_total_loss+=val_loss.item()\n",
    "                    val_total_acc+=val_acc.item()\n",
    "                    \n",
    "            total_loss = total_loss/len(train_loader)\n",
    "            total_acc = total_acc/len(train_loader)\n",
    "            val_total_loss = val_total_loss/len(test_loader)\n",
    "            val_total_acc = val_total_acc/len(test_loader)\n",
    "            \n",
    "            if i%100 == 0:\n",
    "                losses.append(loss)\n",
    "                accur.append(acc)\n",
    "                val_losses.append(val_loss)\n",
    "                print(\"epoch{}\\tloss:{}\\tacc:{}\"\n",
    "                      .format(i,np.round(total_loss,3),np.round(total_acc,3)),\n",
    "                      \"\\tval_loss:{}\\tv_acc:{}\"\n",
    "                      .format(np.round(val_total_loss,3),np.round(val_total_acc,3)))\n",
    "                \n",
    "        h = torch.zeros(1, len(test_feature), h_size, requires_grad=False)\n",
    "        c = torch.zeros(1, len(test_feature), h_size, requires_grad=False)\n",
    "        \n",
    "        y_pre=torch.round(model(torch.cuda.FloatTensor(test_feature),(h.cuda(),c.cuda()))).cpu().detach().numpy()\n",
    "        y_proba=model(torch.cuda.FloatTensor(test_feature),(h.cuda(),c.cuda())).cpu().detach().numpy()\n",
    "        y_label = test_label.values       \n",
    "        \n",
    "        test_acc  = (y_pre==y_label).sum()/len(y_label)\n",
    "        AUROC     = roc_auc_score(y_label, y_proba)\n",
    "        AUPRC     = average_precision_score(y_label, y_proba)\n",
    "        precision = precision_score(y_label, y_pre, pos_label=1)\n",
    "        recall    = recall_score(y_label, y_pre)\n",
    "        f1_score_ = f1_score(y_label, y_pre)\n",
    "\n",
    "        lstm_acc = lstm_acc + [test_acc]\n",
    "        lstm_roc = lstm_roc + [AUROC]\n",
    "        lstm_prc = lstm_prc + [AUPRC]\n",
    "        lstm_pre = lstm_pre + [precision]\n",
    "        lstm_rec = lstm_rec + [recall]\n",
    "        lstm_f1  = lstm_f1  + [f1_score_]  \n",
    "        \n",
    "        #torch.save(model.state_dict(), \"trained_model/\"+d+\"_\"+str(tc_1)+\"_lstm.pt\")\n",
    "        #joblib.dump(scaler, \"trained_model/\"+d+\"_\"+str(tc_1)+\"_scaler_lstm.pkl\")\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"lstm : \")\n",
    "    print(\"  Acc      : \",np.round(np.array(lstm_acc).mean(),3))\n",
    "    print(\"  AUROC    : \",np.round(np.array(lstm_roc).mean(),3))\n",
    "    print(\"  AUPRC    : \",np.round(np.array(lstm_prc).mean(),3))\n",
    "    print(\"  Precision: \",np.round(np.array(lstm_pre).mean(),3))\n",
    "    print(\"  Recall   : \",np.round(np.array(lstm_rec).mean(),3))\n",
    "    print(\"  F1-score : \",np.round(np.array(lstm_f1 ).mean(),3))\n",
    "    print(\"\\n\")\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaist_hc",
   "language": "python",
   "name": "kaist_hc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
