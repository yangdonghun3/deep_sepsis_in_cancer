{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EHRs : \n",
      "LogisticRegression(LR) : \n",
      "  Acc      :  0.7\n",
      "  AUROC    :  0.92\n",
      "  AUPRC    :  0.943\n",
      "  Precision:  1.0\n",
      "  Recall   :  0.625\n",
      "  F1-score :  0.769\n",
      "RandomForestClassifier(RF) : \n",
      "  Acc      :  0.8\n",
      "  AUROC    :  1.0\n",
      "  AUPRC    :  1.0\n",
      "  Precision:  1.0\n",
      "  Recall   :  0.714\n",
      "  F1-score :  0.833\n",
      "\n",
      "\n",
      "EHRs_DrugRel : \n",
      "LogisticRegression(LR) : \n",
      "  Acc      :  1.0\n",
      "  AUROC    :  1.0\n",
      "  AUPRC    :  1.0\n",
      "  Precision:  1.0\n",
      "  Recall   :  1.0\n",
      "  F1-score :  1.0\n",
      "RandomForestClassifier(RF) : \n",
      "  Acc      :  1.0\n",
      "  AUROC    :  1.0\n",
      "  AUPRC    :  1.0\n",
      "  Precision:  1.0\n",
      "  Recall   :  1.0\n",
      "  F1-score :  1.0\n",
      "\n",
      "\n",
      "EHRs_DrugRel_Lab : \n",
      "LogisticRegression(LR) : \n",
      "  Acc      :  0.9\n",
      "  AUROC    :  1.0\n",
      "  AUPRC    :  1.0\n",
      "  Precision:  1.0\n",
      "  Recall   :  0.833\n",
      "  F1-score :  0.909\n",
      "RandomForestClassifier(RF) : \n",
      "  Acc      :  0.7\n",
      "  AUROC    :  1.0\n",
      "  AUPRC    :  1.0\n",
      "  Precision:  1.0\n",
      "  Recall   :  0.625\n",
      "  F1-score :  0.769\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "\n",
    "EHRs_DrugRel_Lab = pd.read_csv(\"preprocessed_data(dummy)/(dummy)EHRs_DrugRel_Lab.csv\")\n",
    "Lab_col=EHRs_DrugRel_Lab.columns[101:136]\n",
    "DrugRel_col=EHRs_DrugRel_Lab.columns[346:1741]\n",
    "EHRs_DrugRel=EHRs_DrugRel_Lab.drop(Lab_col.values,axis=1)\n",
    "EHRs=EHRs_DrugRel.drop(DrugRel_col,axis=1)\n",
    "\n",
    "data={\"EHRs\":EHRs,\"EHRs_DrugRel\":EHRs_DrugRel,\"EHRs_DrugRel_Lab\":EHRs_DrugRel_Lab}\n",
    "\n",
    "for d in data.keys():\n",
    "    data_df=data[d]\n",
    "    print(d,\": \")\n",
    "    data_df=data_df.drop([\"Sepsis_Date\"],axis=1)    \n",
    "    padding = pd.DataFrame(0*np.ones((len(data_df), 1742-len(data_df.columns)-1)))\n",
    "    data_df = pd.concat([data_df,padding],axis=1)\n",
    "    test_data=data_df\n",
    "    test_feature = test_data.drop([\"Label\"], axis=1)\n",
    "    test_label = test_data[[\"Label\"]]\n",
    "    scaler = joblib.load(\"trained_model/\"+d+\"_scaler.pkl\")\n",
    "    test_feature = scaler.transform(test_feature)\n",
    "    \n",
    "    model = joblib.load(\"trained_model/\"+d+\"_lr.pkl\")\n",
    "    \n",
    "    lr_acc = model.score(test_feature, test_label)\n",
    "    lr_roc = roc_auc_score(test_label,model.predict_proba(test_feature)[:,1])\n",
    "    lr_prc = average_precision_score(test_label,model.predict_proba(test_feature)[:,1])\n",
    "    lr_pre = recall_score(test_label,  model.predict(test_feature))\n",
    "    lr_rec = precision_score(test_label, model.predict(test_feature))\n",
    "    lr_f1  =f1_score(y_true=test_label, y_pred =model.predict(test_feature))\n",
    "        \n",
    "    model = joblib.load(\"trained_model/\"+d+\"_rf.pkl\")\n",
    "    \n",
    "    rf_acc = model.score(test_feature, test_label)\n",
    "    rf_roc = roc_auc_score(test_label,model.predict_proba(test_feature)[:,1])\n",
    "    rf_prc = average_precision_score(test_label,model.predict_proba(test_feature)[:,1])\n",
    "    rf_pre = recall_score(test_label,  model.predict(test_feature))\n",
    "    rf_rec = precision_score(test_label, model.predict(test_feature))\n",
    "    rf_f1  =f1_score(y_true=test_label, y_pred =model.predict(test_feature))\n",
    "\n",
    "    print(\"LogisticRegression(LR) : \")\n",
    "    print(\"  Acc      : \",np.round(lr_acc,3)) \n",
    "    print(\"  AUROC    : \",np.round(lr_roc,3))\n",
    "    print(\"  AUPRC    : \",np.round(lr_prc,3))\n",
    "    print(\"  Precision: \",np.round(lr_pre,3))\n",
    "    print(\"  Recall   : \",np.round(lr_rec,3))\n",
    "    print(\"  F1-score : \",np.round(lr_f1 ,3))\n",
    "    \n",
    "    print(\"RandomForestClassifier(RF) : \")\n",
    "    print(\"  Acc      : \",np.round(rf_acc,3)) \n",
    "    print(\"  AUROC    : \",np.round(rf_roc,3))\n",
    "    print(\"  AUPRC    : \",np.round(rf_prc,3))\n",
    "    print(\"  Precision: \",np.round(rf_pre,3))\n",
    "    print(\"  Recall   : \",np.round(rf_rec,3))\n",
    "    print(\"  F1-score : \",np.round(rf_f1 ,3))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spesis",
   "language": "python",
   "name": "spesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
